# -*- coding: utf-8 -*-
"""
nlp1_auto_best_stability.py

1) model3_logit_v2.csvì—ì„œ
   - reliability_flag == "ok"
   - logit_w_stability ìµœëŒ“ê°’ì¸ (state, category) ì¡°í•© 1ê°œ ì„ íƒ

2) df11ì—ì„œ í•´ë‹¹ state & category ë”ë¯¸=1ì¸ ì‹ë‹¹ë§Œ ì‚¬ìš©
   - ì„¸ ì§€ìˆ˜(stability/reliability/loyalty) í•© ìƒìœ„ 10% vs ë‚˜ë¨¸ì§€

3) ë‘ ê·¸ë£¹ ë¦¬ë·°ì—ì„œ ê¸ì • trigram ì¶”ì¶œ í›„ log-odds ê³„ì‚°

4) ì „ì²´ ë¦¬ë·°ë¡œ Word2Vec í•™ìŠµ â†’ trigramì„ ë¸Œëœë“œ/ë©”ë‰´/ê³µê°„ìœ¼ë¡œ ë¶„ë¥˜

ì¶œë ¥:
- {state}_{category}_trigram_logodds_top50.csv
- {state}_{category}_trigram_logodds_top50.png
- {state}_{category}_trigram_top50_labeled.csv
"""

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize, ngrams
from nltk.sentiment import SentimentIntensityAnalyzer
from collections import Counter

from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity


# =========================
# 0. ê¸°ë³¸ ì„¤ì •
# =========================
DF11_FN   = "df11.csv"
REVIEW_FN = "review.parquet"   # review.csvë©´ ì•„ë˜ì—ì„œ ë¶„ê¸° ì²˜ë¦¬ ìˆìŒ

STAB_COL = "stability_score"
REL_COL  = "reliability_score"
LOY_COL  = "loyalty_score"

POS_THRESHOLD = 0.3     # ê¸ì • ë¦¬ë·° ê¸°ì¤€ (VADER compound)
TOP_Q         = 0.9     # ì„¸ ì§€ìˆ˜ í•© ìƒìœ„ 10%
MIN_TOP_COUNT = 5       # top ê·¸ë£¹ì—ì„œ ìµœì†Œ ë“±ì¥ íšŸìˆ˜
TOP_K         = 50      # log-odds ìƒìœ„ ëª‡ ê°œ trigramê¹Œì§€ ë³¼ì§€

MODEL3_FN = "model3_logit_v2.csv"


# =========================
# 1. model3_logit_v2ì—ì„œ ìµœê³  ì•ˆì •ì„± ì¡°í•© ì„ íƒ
# =========================
print(f"ğŸ“‚ model3_logit_v2 ë¡œë“œ ì¤‘: {MODEL3_FN}")
m3 = pd.read_csv(MODEL3_FN)
print(f"   ì´ í–‰ ìˆ˜: {len(m3):,}")

req_cols_m3 = ["state", "category", "logit_w_stability", "reliability_flag"]
missing_m3 = [c for c in req_cols_m3 if c not in m3.columns]
if missing_m3:
    raise ValueError(f"âŒ model3_logit_v2ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_m3}")

ok_m3 = m3[m3["reliability_flag"] == "ok"].copy()
if ok_m3.empty:
    raise ValueError("âŒ reliability_flag == 'ok' ì¸ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")

best_idx = ok_m3["logit_w_stability"].idxmax()
best_row = ok_m3.loc[best_idx]

STATE_TARGET   = str(best_row["state"]).upper()
CATEGORY_NAME  = str(best_row["category"])
CAT_COL        = f"c_{CATEGORY_NAME}"

tag = f"{STATE_TARGET.lower()}_{CATEGORY_NAME.lower()}"

OUT_LOGODDS_CSV  = f"{tag}_trigram_logodds_top50.csv"
OUT_PNG          = f"{tag}_trigram_logodds_top50.png"
OUT_LABELED_CSV  = f"{tag}_trigram_top50_labeled.csv"

print("\nğŸ† ì„ íƒëœ ì¡°í•© (ìµœê³  logit_w_stability & reliability_flag='ok'):")
print(best_row[["state", "category", "logit_w_stability", "logit_w_reliability",
                "logit_w_loyalty", "reliability_flag"]])
print(f"\nğŸ¯ ëŒ€ìƒ state: {STATE_TARGET}, category: {CATEGORY_NAME} (ë”ë¯¸ ì»¬ëŸ¼: {CAT_COL})")


# =========================
# 2. NLTK ë¦¬ì†ŒìŠ¤ ì²´í¬ & ê¸°ë³¸ ê°ì²´
# =========================
def ensure_nltk_resources():
    needed = [
        ("punkt", "tokenizers/punkt"),
        ("stopwords", "corpora/stopwords"),
        ("vader_lexicon", "sentiment/vader_lexicon"),
    ]
    for pkg, path in needed:
        try:
            nltk.data.find(path)
        except LookupError:
            nltk.download(pkg)

ensure_nltk_resources()

# trigramìš© stopwords: ê¸°ëŠ¥ì–´ ìœ„ì£¼ ì œê±°, ê°ì • ë‹¨ì–´ëŠ” ì‚´ë¦¼
stop_words = set(stopwords.words("english"))
emotion_keep = {"good", "great", "love", "amazing", "best", "really", "so", "very", "favorite"}
stop_words = stop_words - emotion_keep

sia = SentimentIntensityAnalyzer()


# =========================
# 3. ê¸ì • trigram ì¶”ì¶œ í•¨ìˆ˜
# =========================
def get_positive_trigrams(df_reviews, text_col="text",
                          pos_threshold=0.3, stop_words=None):
    """
    - ë¦¬ë·° í…ìŠ¤íŠ¸ ì¤‘ VADER compound > pos_threshold ì¸ ê¸ì • ë¦¬ë·°ë§Œ ì‚¬ìš©
    - í† í°í™” + stopword ì œê±° í›„ trigram ìƒì„±
    """
    if stop_words is None:
        stop_words = set()

    trigram_list = []
    n_pos = 0

    for text in df_reviews[text_col]:
        if not isinstance(text, str):
            continue

        score = sia.polarity_scores(text)["compound"]
        if score <= pos_threshold:
            continue

        n_pos += 1

        tokens = [
            w.lower()
            for w in word_tokenize(text)
            if w.isalpha() and w.lower() not in stop_words
        ]

        trigram_list.extend(list(ngrams(tokens, 3)))

    print(f"   ê¸ì • ë¦¬ë·° ìˆ˜: {n_pos:,} / trigram ìˆ˜: {len(trigram_list):,}")
    return trigram_list


# =========================
# 4. df11: ì„ íƒëœ state & category ë”ë¯¸=1 ìƒìœ„10% / ë‚˜ë¨¸ì§€ ë¶„ë¦¬
# =========================
print(f"\nğŸ“‚ df11 ë¡œë“œ ì¤‘: {DF11_FN}")
df = pd.read_csv(DF11_FN)
print(f"   ì´ í–‰ ìˆ˜: {len(df):,}")

required_cols_df11 = ["business_id", "state", STAB_COL, REL_COL, LOY_COL, CAT_COL]
missing_df11 = [c for c in required_cols_df11 if c not in df.columns]
if missing_df11:
    raise ValueError(f"âŒ df11ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_df11}")

mask_state = df["state"].astype(str).str.upper() == STATE_TARGET
mask_cat   = df[CAT_COL] == 1
df_target = df[mask_state & mask_cat].copy()

print(f"\nğŸ¯ ëŒ€ìƒ: {STATE_TARGET} & {CAT_COL}=1 ì‹ë‹¹ ìˆ˜: {len(df_target):,}")
if df_target.empty:
    raise ValueError("âŒ í•´ë‹¹ state & category ë”ë¯¸=1ì¸ ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.")

df_target["sum_3idx"] = (
    df_target[STAB_COL] +
    df_target[REL_COL] +
    df_target[LOY_COL]
)

cutoff = df_target["sum_3idx"].quantile(TOP_Q)
df_top  = df_target[df_target["sum_3idx"] >= cutoff].copy()
df_rest = df_target[df_target["sum_3idx"] < cutoff].copy()

print(f"   ì„¸ ì§€ìˆ˜ í•© ìƒìœ„ {int(TOP_Q*100)}% cutoff: {cutoff:.4f}")
print(f"   ìƒìœ„ ê·¸ë£¹ ë§¤ì¥ ìˆ˜: {len(df_top):,}")
print(f"   ë‚˜ë¨¸ì§€ ê·¸ë£¹ ë§¤ì¥ ìˆ˜: {len(df_rest):,}")

top_ids  = df_top["business_id"].unique().tolist()
rest_ids = df_rest["business_id"].unique().tolist()


# =========================
# 5. ë¦¬ë·° ë¡œë“œ & ê·¸ë£¹ ë¶„ë¦¬
# =========================
print(f"\nğŸ“‚ ë¦¬ë·° ë¡œë“œ ì¤‘: {REVIEW_FN}")
if REVIEW_FN.lower().endswith(".parquet"):
    reviews = pd.read_parquet(REVIEW_FN)
else:
    reviews = pd.read_csv(REVIEW_FN)

req_rev_cols = ["business_id", "text"]
missing_rev = [c for c in req_rev_cols if c not in reviews.columns]
if missing_rev:
    raise ValueError(f"âŒ ë¦¬ë·° íŒŒì¼ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_rev}")

reviews_target = reviews[reviews["business_id"].isin(df_target["business_id"])].copy()
print(f"   ëŒ€ìƒ ë§¤ì¥ ë¦¬ë·° ìˆ˜: {len(reviews_target):,}")

reviews_top  = reviews_target[reviews_target["business_id"].isin(top_ids)].copy()
reviews_rest = reviews_target[reviews_target["business_id"].isin(rest_ids)].copy()

print(f"   â–¶ ìƒìœ„ ê·¸ë£¹ ë¦¬ë·° ìˆ˜: {len(reviews_top):,}")
print(f"   â–¶ ë‚˜ë¨¸ì§€ ê·¸ë£¹ ë¦¬ë·° ìˆ˜: {len(reviews_rest):,}")

if reviews_top.empty or reviews_rest.empty:
    raise ValueError("âŒ ìƒìœ„/ë‚˜ë¨¸ì§€ ê·¸ë£¹ ë¦¬ë·°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")


# =========================
# 6. trigram ì¶”ì¶œ (top / rest)
# =========================
print("\nğŸ§  ìƒìœ„ ê·¸ë£¹ trigram ì¶”ì¶œ ì¤‘...")
trigrams_top = get_positive_trigrams(
    reviews_top,
    text_col="text",
    pos_threshold=POS_THRESHOLD,
    stop_words=stop_words,
)

print("\nğŸ§  ë‚˜ë¨¸ì§€ ê·¸ë£¹ trigram ì¶”ì¶œ ì¤‘...")
trigrams_rest = get_positive_trigrams(
    reviews_rest,
    text_col="text",
    pos_threshold=POS_THRESHOLD,
    stop_words=stop_words,
)

if not trigrams_top or not trigrams_rest:
    raise ValueError("âŒ trigramì´ í•œìª½ ê·¸ë£¹ì—ì„œ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")


# =========================
# 7. log-odds ê³„ì‚°
# =========================
print("\nğŸ“Š log-odds ê³„ì‚° ì¤‘...")

fdist_top  = Counter(trigrams_top)
fdist_rest = Counter(trigrams_rest)

all_trigrams = set(fdist_top.keys()) | set(fdist_rest.keys())

rows = []
for tg in all_trigrams:
    top_freq  = fdist_top[tg]
    rest_freq = fdist_rest[tg]

    if top_freq < MIN_TOP_COUNT:
        continue

    # ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©
    top_s  = top_freq + 1
    rest_s = rest_freq + 1

    log_odds = np.log(top_s / rest_s)
    phrase = " ".join(tg)
    rows.append((tg, phrase, top_freq, rest_freq, log_odds))

df_logodds = pd.DataFrame(
    rows,
    columns=["trigram", "phrase", "count_top", "count_rest", "log_odds"]
).sort_values("log_odds", ascending=False).reset_index(drop=True)

print("\n=== ğŸ“Š log-odds ìƒìœ„ 10ê°œ ì˜ˆì‹œ ===")
print(df_logodds.head(10))

# ìƒìœ„ 50ê°œë§Œ ì €ì¥ (ë¶„ì„ìš©)
df_topk = df_logodds.head(TOP_K).copy()
df_topk.to_csv(OUT_LOGODDS_CSV, index=False, encoding="utf-8-sig")
print(f"\nâœ… log-odds ìƒìœ„ {TOP_K}ê°œ ì €ì¥ ì™„ë£Œ: {OUT_LOGODDS_CSV}")

# ê°„ë‹¨ ì‹œê°í™”
plt.figure(figsize=(10, 6))
df_plot = df_topk.sort_values("log_odds", ascending=True)
plt.barh(df_plot["phrase"], df_plot["log_odds"])
plt.xlabel("log-odds (Top vs Rest)")
plt.ylabel("Trigram")
plt.title(f"{STATE_TARGET} - {CATEGORY_NAME} (ë”ë¯¸: {CAT_COL}=1) - Top {TOP_K} trigram log-odds")
plt.tight_layout()
plt.savefig(OUT_PNG, dpi=150)
plt.show()
print(f"ğŸ“ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {OUT_PNG}")


# =========================
# 8. Word2Vec í•™ìŠµ (í•´ë‹¹ state+category ì „ì²´ ë¦¬ë·° ê¸°ë°˜)
# =========================
print("\nğŸ§¬ Word2Vec í•™ìŠµìš© ì½”í¼ìŠ¤ ì¤€ë¹„ ì¤‘...")

sentences = []
for text in reviews_target["text"]:
    if not isinstance(text, str):
        continue
    tokens = [
        w.lower()
        for w in word_tokenize(text)
        if w.isalpha()
    ]
    if tokens:
        sentences.append(tokens)

print(f"   ë¬¸ì¥(ë¦¬ë·°) ìˆ˜: {len(sentences):,}")

print("ğŸ§¬ Word2Vec í•™ìŠµ ì¤‘...")
w2v_model = Word2Vec(
    sentences=sentences,
    vector_size=100,
    window=5,
    min_count=5,
    workers=4,
    sg=1,          # skip-gram
    epochs=10
)
print("âœ… Word2Vec í•™ìŠµ ì™„ë£Œ")


# =========================
# 9. trigram ì„ë² ë”© + ë¸Œëœë“œ/ë©”ë‰´/ê³µê°„ ë¶„ë¥˜
# =========================

# === 1) seed words (brand ì œê±°, 4ì¶•ë§Œ ì‚¬ìš©) ===
menu_seeds  = ["ramen","soba","noodles","tikka","curry","dumpling","burger","wings",
               "pancakes","omelette","sandwich","salad","brunch","pho","bibimbap","taco"]
space_seeds = ["patio","view","inside","outside","rooftop","terrace","seating",
               "atmosphere","ambience","vibe","counter","bar","booth"]
ops_seeds   = ["seasonal","rotating","rotate","special","chef","daily","today",
               "weekly","tasting","limited","pop-up","prefix","prix","course"]
regi_seeds  = ["korean","japanese","thai","vietnamese","mexican","sicilian","tuscan",
               "philly","chicago","nashville","texas","cajun","bavarian","peruvian",
               "savoy","neapolitan","szechuan","hunan"]

def get_seed_vector(seed_words, model):
    vecs = [model.wv[w] for w in seed_words if w in model.wv]
    return np.mean(vecs, axis=0) if vecs else None

menu_vec  = get_seed_vector(menu_seeds,  w2v_model)
space_vec = get_seed_vector(space_seeds, w2v_model)
ops_vec   = get_seed_vector(ops_seeds,   w2v_model)
regi_vec  = get_seed_vector(regi_seeds,  w2v_model)

# === 2) ë£°/í‚¤ì›Œë“œ íŒ¨í„´ (operation & regional) ===
OPS_PATTERNS = [
    "menu changes", "rotating menu", "seasonal menu", "chef special", "chef's choice",
    "tasting menu", "daily special", "today special", "limited time", "limited edition",
    "weekly special", "prix fixe", "prefix menu", "course menu", "pop-up", "only on", "weekend only"
]
REGI_PATTERNS = [
    # êµ­ì /ì§€ì—­/ë„ì‹œ/ìŠ¤íƒ€ì¼ í‚¤ì›Œë“œ(í•„ìš”ì‹œ ì¶”ê°€)
    "korean", "japanese", "thai", "vietnamese", "mexican", "sicilian", "tuscan",
    "philly", "philadelphia", "chicago", "nashville", "texas", "tex-mex", "cajun",
    "neapolitan", "szechuan", "hunan", "bavarian", "peruvian", "hawaiian"
]

def has_any(text: str, patterns: list[str]) -> bool:
    t = text.lower()
    return any(p in t for p in patterns)

from sklearn.metrics.pairwise import cosine_similarity

# === 3) 4ì¶• ë¶„ë¥˜ê¸° (brand ì œê±°) ===
def classify_trigram_four(phrase: str, model, menu_vec, space_vec, ops_vec, regi_vec):
    toks = phrase.split()
    vecs = [model.wv[t] for t in toks if t in model.wv]
    # ì„ë² ë”©ì´ ì „í˜€ ì—†ìœ¼ë©´ ë£°ë¡œë§Œ íŒë‹¨
    if not vecs:
        if has_any(phrase, OPS_PATTERNS):  return "operation", None, None, None, None
        if has_any(phrase, REGI_PATTERNS): return "regional",  None, None, None, None
        # ë©”ë‰´/ê³µê°„ì€ ë£° ì •ì˜ê°€ ì• ë§¤í•˜ë©´ unknown ì²˜ë¦¬
        return "unknown", None, None, None, None

    v = np.mean(vecs, axis=0)
    sim = lambda a,b: float(cosine_similarity(a.reshape(1,-1), b.reshape(1,-1))[0][0]) if a is not None and b is not None else -999

    sims = {
        "menu":      sim(v, menu_vec),
        "space":     sim(v, space_vec),
        "operation": sim(v, ops_vec),
        "regional":  sim(v, regi_vec),
    }

    # ë£° ê°€ì‚°(íœ´ë¦¬ìŠ¤í‹±)
    if has_any(phrase, OPS_PATTERNS):
        sims["operation"] += 0.15
    if has_any(phrase, REGI_PATTERNS):
        sims["regional"]  += 0.15

    best_type = max(sims, key=sims.get)
    return best_type, sims["menu"], sims["space"], sims["operation"], sims["regional"]

# === 4) ë¼ë²¨ë§ ì‹¤í–‰ ===
labels, s_menu, s_space, s_ops, s_regi = [], [], [], [], []
for phrase in df_topk["phrase"]:
    t, smn, ssp, sop, srg = classify_trigram_four(
        phrase, w2v_model, menu_vec, space_vec, ops_vec, regi_vec
    )
    labels.append(t); s_menu.append(smn); s_space.append(ssp); s_ops.append(sop); s_regi.append(srg)

df_topk["type"]         = labels
df_topk["sim_menu"]     = s_menu
df_topk["sim_space"]    = s_space
df_topk["sim_operation"]= s_ops
df_topk["sim_regional"] = s_regi

# === 5) ìš”ì•½ ì§€í‘œ(ë¹„ì¤‘/ê°•ë„) ===
def share_and_intensity(df, label):
    m = df["type"] == label
    return m.mean(), df.loc[m, "log_odds"].mean()

for lab in ["menu","space","regional","operation"]:
    sh, inten = share_and_intensity(df_topk, lab)
    print(f"[{lab}] share={sh:.2%}, intensity={inten if pd.notna(inten) else float('nan'):.3f}")


print("\n=== ğŸ·ï¸ ìƒìœ„ 20ê°œ trigram + íƒ€ì… ì˜ˆì‹œ ===")
print(df_topk.head(20)[["phrase", "type", "sim_menu", "sim_space", "sim_operation", "sim_regional"]])

df_topk.to_csv(OUT_LABELED_CSV, index=False, encoding="utf-8-sig")
print(f"\nâœ… ë¼ë²¨ë§ í¬í•¨ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {OUT_LABELED_CSV}")

print("\nğŸ‰ ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ!")
