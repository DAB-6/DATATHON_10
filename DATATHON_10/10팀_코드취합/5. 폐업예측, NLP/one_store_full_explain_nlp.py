# -*- coding: utf-8 -*-
"""
one_store_full_explain_nlp.py

âœ” ëª©ì 
- (1) CatBoost ë¶„ë¥˜ ëª¨ë¸ë¡œ ê°œë³„ ë§¤ì¥ì˜ íì—… ìœ„í—˜ í™•ë¥  ì˜ˆì¸¡
- (2) ë¡œì»¬ SHAPìœ¼ë¡œ ì£¼ìš” ê¸°ì—¬ ìš”ì¸ Top-N ì„¤ëª… ë° %í¬ì¸íŠ¸ ì˜í–¥ëŸ‰ ì‚°ì¶œ
- (3) ì „ì²´ ë¶„í¬ ëŒ€ë¹„ í•´ë‹¹ ë§¤ì¥ì˜ ìœ„í—˜ ë°±ë¶„ìœ„(ìƒìœ„ ëª‡ %) ê³„ì‚°
- (4) ë‹¨ì¼ ì‹ë‹¹ NLP íŒŒì´í”„ë¼ì¸(ê°ì„±/ë¡œê·¸ì˜¤ì¦ˆ/ì¸¡ë©´/ìš”ì•½ì¹´ë“œ) ì‹¤í–‰

ì…ë ¥
- df11.csv (ë©”íƒ€/ì¹´í…Œê³ ë¦¬/ì§€ìˆ˜ í¬í•¨)
- review_filtered.parquet (ë¦¬ë·° í…ìŠ¤íŠ¸)
- catboost_df11_close_real_bestF1.cbm (í•™ìŠµëœ ëª¨ë¸)

ì¶œë ¥
- one_store_explain.csv (SHAP + í™•ë¥  ë³€í™” %p + ë©”íƒ€)
- one_store_distribution.csv (ì „ì²´ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ + ë°±ë¶„ìœ„)
- nlp_out/ (ì›”ë³„ ê°ì„±, ë¶€ì • bi/tri-gram ë¡œê·¸ì˜¤ì¦ˆ, ì¸¡ë©´ ì ìˆ˜, ì˜ˆì‹œ ë¬¸ì¥, ìš”ì•½ì¹´ë“œ)

ì‚¬ìš©ë²•(ì˜ˆ)
- BUSINESS_ID ì§€ì •:
    BUSINESS_ID = "5Md0YaxD5HiOoBmsnmIu7A"
- ë˜ëŠ” ìë™ ì„ íƒ(ë³„ì â†“, ê²½ìŸë„â†‘, ì•ˆì •ì§€ìˆ˜â†“ ì¡°ê±´ìœ¼ë¡œ í›„ë³´â†’ìµœê³  ìœ„í—˜ 1ê°œ)

ë©”ëª¨
- BEST_THRë¥¼ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ best_thrë¡œ ì§€ì •í•˜ë©´ íŒì • ì¼ê´€ì„±ì´ ì˜¬ë¼ê°‘ë‹ˆë‹¤.
- SCORE ìŠ¤ì¼€ì¼(ì„ íƒ): ë‹¨ìˆœí˜•(100Ã—(1âˆ’p)), ë¡œê·¸í˜•(100Ã—(1âˆ’âˆšp)) ë™ì‹œì— ê³„ì‚°í•´ ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import re
import math
from pathlib import Path
from collections import Counter

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from catboost import CatBoostClassifier, Pool
import shap
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# =========================
# 0) ì„¤ì •
# =========================
DF_PATH      = "df11.csv"
MODEL_PATH   = "catboost_df11_close_real_bestF1.cbm"
REVIEWS_PATH = "review_filtered.parquet"
OUT_DIR_NLP  = Path("nlp_out")

# ê°œë³„ ì‹¤í–‰ ì˜µì…˜
BUSINESS_ID = None  # ì˜ˆ: "5Md0YaxD5HiOoBmsnmIu7A" (Noneì´ë©´ ìë™ ì„ íƒ ë¡œì§)
BEST_THR    = 0.87  # ì˜ˆ: 0.23 (Noneì´ë©´ 0.50)
TOP_N       = 8     # SHAP ìƒ/í•˜ìœ„ í‘œì‹œ ê°œìˆ˜

# ğŸ”€ í›„ë³´ ì„ íƒ ëª¨ë“œ
# - "argmax": ê°€ì¥ ìœ„í—˜(í™•ë¥  ìµœëŒ€) 1ê°œ
# - "random_topk": ìƒìœ„ TOPK ì¤‘ ë¬´ì‘ìœ„ 1ê°œ
# - "weighted_softmax": í™•ë¥  ê°€ì¤‘ ì†Œí”„íŠ¸ë§¥ìŠ¤ ìƒ˜í”Œë§(í™•ë¥  ë†’ì„ìˆ˜ë¡ ì„ íƒâ†‘)
# - "pure_random": í•„í„° í†µê³¼ í›„ë³´ ì¤‘ ì™„ì „ ëœë¤
PICK_MODE    = "random_topk"
TOPK         = 50         # random_topkì—ì„œ ì‚¬ìš©í•˜ëŠ” ìƒìœ„ ê°œìˆ˜
TEMPERATURE  = 0.7        # weighted_softmaxì—ì„œ ì˜¨ë„(â†“ì´ë©´ ìƒìœ„ì— ë” ì§‘ì¤‘)
RANDOM_STATE = 42         # ì¬í˜„ì„±ìš© ì‹œë“œ(ì›í•˜ë©´ None)

# ğŸ” ì¤‘ë³µ ë°©ì§€: ì´ì „ì— ë½‘íŒ ë§¤ì¥ ì œì™¸ìš© ë¡œì»¬ íŒŒì¼
EXCLUDE_IDS_PATH = Path("picked_ids.txt")

# ë¶„í¬ ë°±ë¶„ìœ„ ê³„ì‚° ì˜µì…˜ (ì „ì²´ df11 ëŒ€ìƒ ì˜ˆì¸¡ ìˆ˜í–‰)
COMPUTE_GLOBAL_DIST = True

# ë¶„í¬ ë°±ë¶„ìœ„ ê³„ì‚° ì˜µì…˜ (ì „ì²´ df11 ëŒ€ìƒ ì˜ˆì¸¡ ìˆ˜í–‰)
COMPUTE_GLOBAL_DIST = True

# =========================
# 1) ê³µí†µ ìœ í‹¸
# =========================

def ensure_cols(df: pd.DataFrame, need: set, name: str):
    miss = need - set(df.columns.astype(str))
    if miss:
        raise ValueError(f"{name}ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {sorted(miss)}")


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def to_scores(p: float) -> dict:
    """í™•ë¥ â†’ì ìˆ˜ ìŠ¤ì¼€ì¼ ë³€í™˜(ë‹¨ìˆœí˜•/ë¡œê·¸í˜• ë‘˜ ë‹¤)"""
    p = float(p)
    score_simple = 40 + 60 * (1 - p) ** (0.5) 
    score_log    = (1 - math.sqrt(p)) * 100.0
    return {
        "score_simple": score_simple,
        "score_log": score_log,
    }


# =========================
# 2) ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬(í•™ìŠµê³¼ ë™ì¼)
# =========================

df = pd.read_csv(DF_PATH)
ensure_cols(df, {"store_status"}, "df11")

# í•™ìŠµ ì‹œ ë°˜ì˜í–ˆë˜ ê°„ë‹¨ ì „ì²˜ë¦¬(í•„ìš” ì»¬ëŸ¼ë§Œ ì•ˆì „ ì ìš©)
bool_cols = [
    "a_outdoor_seating", "a_good_for_group", "a_good_for_kids",
    "a_has_tv", "a_happy_hour"
]
for c in bool_cols:
    if c in df.columns:
        df[c] = df[c].fillna(0).astype(int)

noise_mapping = {"quiet": 0, "average": 1, "loud": 2, "very_loud": 3}
if "a_noise_level" in df.columns:
    df["a_noise_level"] = df["a_noise_level"].map(noise_mapping)

if "a_alcohol" in df.columns and "c_nightlife" in df.columns:
    df["a_alcohol"] = np.where(
        df["a_alcohol"].isna(),
        np.where(df["c_nightlife"] == 1, "full_bar", "none"),
        df["a_alcohol"]
    )
if "a_ambience" in df.columns:
    df["a_ambience"] = df["a_ambience"].fillna("unknown")

# get_dummies (store_status í¬í•¨: íƒ€ê¹ƒ ë”ë¯¸ ìƒì„±ë˜ì§€ë§Œ Xì—ëŠ” ì œì™¸)
df_dum = pd.get_dummies(
    df.copy(),
    columns=["store_status", "a_alcohol", "a_ambience"],
    prefix=["status", "alcohol", "ambience"],
)

TARGET_COL = "status_close_real"
if TARGET_COL not in df_dum.columns:
    raise ValueError("status_close_real ë”ë¯¸ê°€ ì—†ìŠµë‹ˆë‹¤. get_dummies ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

exclude_cols = [
    "business_id", "name", "address", "city", "attributes", "categories", "hours",
    "is_open", "latitude", "longitude", TARGET_COL,
    "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday",
    "review_count"
]
status_cols = [c for c in df_dum.columns if c.startswith("status_")]
exclude_cols += [c for c in status_cols if c not in exclude_cols]

feature_cols = [c for c in df_dum.columns if c not in exclude_cols]
X_all = df_dum[feature_cols].copy()

# CatBoost ë²”ì£¼í˜• ì²˜ë¦¬(ë¬¸ìí˜• ìœ ì§€)
cat_cols = [c for c in ["state", "postal_code"] if c in X_all.columns]
for col in X_all.columns:
    if col in cat_cols:
        X_all[col] = X_all[col].astype(str).fillna("missing")
    else:
        X_all[col] = X_all[col].fillna(0)

cat_feature_indices = [X_all.columns.get_loc(c) for c in cat_cols]

# =========================
# 3) ëª¨ë¸ ë¡œë“œ
# =========================
model = CatBoostClassifier()
model.load_model(MODEL_PATH)

# =========================
# 4) íƒ€ê¹ƒ ë§¤ì¥ ì„ íƒ(ì§€ì • or ìë™)
# =========================
if BUSINESS_ID is not None and "business_id" in df.columns:
    row_sel = df.loc[df["business_id"] == BUSINESS_ID]
    if row_sel.empty:
        raise ValueError("í•´ë‹¹ business_idê°€ df11ì— ì—†ìŠµë‹ˆë‹¤.")
    idx = row_sel.index[0]
else:
    # ìë™ ì„ íƒ: ì˜¤í”ˆë§¤ì¥ & ë³„ì â†“ & ê²½ìŸë„â†‘ & ì•ˆì •ì§€ìˆ˜â†“ ì¡°ê±´
    conds = [df["is_open"] == 1] if "is_open" in df.columns else []
    if "stars" in df.columns:
        conds.append(df["stars"] <= df["stars"].quantile(0.3))
    if "neighbor_density" in df.columns:
        conds.append(df["neighbor_density"] >= df["neighbor_density"].quantile(0.7))
    if "stability_score" in df.columns:
        conds.append(df["stability_score"] <= df["stability_score"].quantile(0.3))
    # âœ… ë¦¬ë·° 100ê°œ ì´ìƒ í•„í„°
    if "review_count" in df.columns:
        conds.append(df["review_count"] >= 100)

    candidates = df[np.logical_and.reduce(conds)] if conds else df.copy()
    if candidates.empty:
        candidates = df[(df.get("is_open", 1) == 1) & (df.get("review_count", 0) >= 100)].nsmallest(50, "stability_score") if "stability_score" in df.columns else df[(df.get("is_open", 1) == 1) & (df.get("review_count", 0) >= 100)].sample(50, random_state=42)

    X_cand  = X_all.loc[candidates.index]
pool_cd = Pool(X_cand, cat_features=cat_feature_indices)
proba_cand = model.predict_proba(pool_cd)[:, 1]

# ===== ì„ íƒ ì „ëµ =====
np.random.seed(RANDOM_STATE if RANDOM_STATE is not None else None)

# ì´ì „ì— ì„ íƒí•œ business_id ì œì™¸(ê°€ëŠ¥í•  ë•Œ)
exclude_ids = set()
if EXCLUDE_IDS_PATH.exists():
    try:
        with open(EXCLUDE_IDS_PATH, "r", encoding="utf-8") as f:
            exclude_ids = set([line.strip() for line in f if line.strip()])
    except Exception:
        exclude_ids = set()

cand_index = X_cand.index
if "business_id" in df.columns and len(exclude_ids) > 0:
    keep_mask = ~df.loc[cand_index, "business_id"].astype(str).isin(exclude_ids)
    if keep_mask.any():
        cand_index = cand_index[keep_mask]
        proba_cand = proba_cand[keep_mask.values]

if len(cand_index) == 0:
    cand_index = X_cand.index  # ëª¨ë‘ ì œì™¸ëë‹¤ë©´ ì›ë³µ

if PICK_MODE == "argmax":
    sel_pos = int(np.argmax(proba_cand))
    idx = int(cand_index[sel_pos])
elif PICK_MODE == "random_topk":
    # ìƒìœ„ TOPK ì•ˆì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ(í›„ë³´ ìˆ˜ê°€ TOPKë³´ë‹¤ ì‘ìœ¼ë©´ ê°€ëŠ¥í•œ ë²”ìœ„)
    order = np.argsort(-proba_cand)
    k = min(TOPK, len(order))
    top_idx = order[:k]
    sel_pos = np.random.choice(top_idx)
    idx = int(cand_index[sel_pos])
elif PICK_MODE == "weighted_softmax":
    # ì†Œí”„íŠ¸ë§¥ìŠ¤ ê°€ì¤‘ì¹˜ ìƒ˜í”Œë§
    # ì˜¨ë„ T: p_i âˆ exp(logit_i / T); ì—¬ê¸°ì„œëŠ” logit ëŒ€ì‹  í™•ë¥ ì„ ì‚¬ìš©
    # í™•ë¥ ì´ í° í•­ëª©ì˜ ì„ íƒ í™•ë¥ ì„ ë†’ì„
    logits = proba_cand / max(TEMPERATURE, 1e-6)
    # ì•ˆì •ì  softmax
    m = np.max(logits)
    w = np.exp(logits - m)
    w = w / (w.sum() + 1e-12)
    sel_pos = np.random.choice(np.arange(len(cand_index)), p=w)
    idx = int(cand_index[sel_pos])
elif PICK_MODE == "pure_random":
    idx = int(np.random.choice(cand_index))
else:
    # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œë©´ ì•ˆì „í•˜ê²Œ argmax
    sel_pos = int(np.argmax(proba_cand))
    idx = int(cand_index[sel_pos])

# ì„ íƒí•œ business_id ê¸°ë¡(ì¤‘ë³µ ë°©ì§€ìš©)
try:
    if "business_id" in df.columns:
        picked_id = str(df.loc[idx, "business_id"])  # type: ignore
        with open(EXCLUDE_IDS_PATH, "a", encoding="utf-8") as f:
            f.write(picked_id + "\n")

except Exception:
    pass

# =========================
# 5) ê°œë³„ ì˜ˆì¸¡ + SHAP
# =========================
x_row = X_all.loc[[idx]].copy()
pool_row = Pool(x_row, cat_features=cat_feature_indices)
proba = float(model.predict_proba(pool_row)[:, 1][0])

thr = 0.5 if BEST_THR is None else float(BEST_THR)
pred = int(proba >= thr)

# CatBoost ShapValues: (n_samples, n_features+1) ë§ˆì§€ë§‰ì—´ base logit
shap_vals_full = model.get_feature_importance(type="ShapValues", data=pool_row)
base_logit = float(shap_vals_full[0, -1])
shap_vals  = shap_vals_full[:, :-1]
contrib    = pd.Series(shap_vals[0], index=x_row.columns).sort_values(ascending=False)

# í™•ë¥  ë³€í™”(%í¬ì¸íŠ¸) ê·¼ì‚¬
base_prob = sigmoid(base_logit)

def shap_to_pct_point(s):
    new_prob = sigmoid(base_logit + s)
    return (new_prob - base_prob) * 100.0

impact_pct = contrib.apply(shap_to_pct_point)

# ë©”íƒ€ ìˆ˜ì§‘
meta_cols = [
    "business_id", "name", "state", "city", "stars", "review_count",
    "stability_score", "loyalty_score", "reliability_score", "neighbor_density"
]
meta_cols = [c for c in meta_cols if c in df.columns]
meta = df.loc[idx, meta_cols].to_dict()

# ì ìˆ˜ ìŠ¤ì¼€ì¼ ì¶”ê°€
score_dict = to_scores(proba)

# =========================
# 6) ì „ì²´ ë¶„í¬ ì˜ˆì¸¡ â†’ ë°±ë¶„ìœ„(ìƒìœ„ ëª‡ %) ê³„ì‚°
# =========================
percentile_val = None
rank_pct = None

if COMPUTE_GLOBAL_DIST:
    pool_all = Pool(X_all, cat_features=cat_feature_indices)
    all_proba = model.predict_proba(pool_all)[:, 1]
    dist_df = df[["business_id"]].copy() if "business_id" in df.columns else pd.DataFrame(index=df.index)
    dist_df["pred_proba"] = all_proba
    # ë°±ë¶„ìœ¨ ë­í¬(ë‚®ìŒâ†’ë†’ìŒ)
    dist_df["risk_percentile"] = dist_df["pred_proba"].rank(pct=True) * 100.0
    # í˜„ì¬ idx ìœ„ì¹˜ ê°’
    rank_pct = float(dist_df.loc[idx, "risk_percentile"]) if idx in dist_df.index else None
    percentile_val = 100.0 - rank_pct if rank_pct is not None else None  # ìƒìœ„ x% í•´ì„ìš©(í° í™•ë¥ ì´ ìƒìœ„)

    # ì ìˆ˜ ìŠ¤ì¼€ì¼ë„ ê°™ì´ ì €ì¥
    dist_df["score_simple"] = (1 - dist_df["pred_proba"]) * 100.0
    dist_df["score_log"]    = (1 - np.sqrt(dist_df["pred_proba"])) * 100.0

    dist_df.to_csv("one_store_distribution.csv", index=False, encoding="utf-8-sig")

# =========================
# 7) ê²°ê³¼ CSV ì €ì¥ (ê°œë³„ ì‹ë‹¹ ê¸°ì¤€ í…Œì´ë¸”)
# =========================
out = pd.DataFrame({
    "feature": contrib.index,
    "shap_value": contrib.values,
    "impact_pct_point": impact_pct.values
})
out.insert(0, "business_index", idx)
out.insert(1, "pred_proba", proba)
out.insert(2, "pred_label", pred)
out.insert(3, "base_logit", base_logit)
out.insert(4, "base_prob", base_prob)

# ì ìˆ˜/ë°±ë¶„ìœ„ ë©”íƒ€ ì—´ ì¶”ê°€
out.insert(5, "score_simple", score_dict["score_simple"])  # 100Ã—(1âˆ’p)
out.insert(6, "score_log", score_dict["score_log"])        # 100Ã—(1âˆ’âˆšp)
if rank_pct is not None:
    out.insert(7, "risk_percentile", rank_pct)              # í° ê°’ì¼ìˆ˜ë¡ ìœ„í—˜ ìƒìœ„
    out.insert(8, "risk_top_percent", 100.0 - rank_pct)     # "ìƒìœ„ ëª‡ %"(ì‘ì„ìˆ˜ë¡ ìœ„í—˜ ìƒìœ„)

# ë©”íƒ€ ì¶”ê°€
for k, v in meta.items():
    out[k] = v

out.to_csv("one_store_explain.csv", index=False, encoding="utf-8-sig")

# ê°„ë‹¨ ì½˜ì†” ìš”ì•½
print("\n==== [ê°œë³„ ì‹ë‹¹ íì—… ìœ„í—˜ ì˜ˆì¸¡] ====")
print(f"- ëŒ€ìƒ ì¸ë±ìŠ¤: {idx}")
for k, v in meta.items():
    print(f"  {k}: {v}")
print(f"- ì˜ˆì¸¡ íì—… í™•ë¥ : {proba:.4f}  (ì ìˆ˜: simple={score_dict['score_simple']:.1f}, log={score_dict['score_log']:.1f})")
print(f"- íŒì •(thr={thr:.2f}): {'íì—…(1)' if pred==1 else 'ë¹„íì—…(0)'}")
if rank_pct is not None:
    print(f"- ìœ„í—˜ ë°±ë¶„ìœ„ ë­í¬: {rank_pct:.2f}%  â†’ ìƒìœ„ {100.0-rank_pct:.2f}% ìœ„í—˜")

print("\n[ë¡œì»¬ SHAP ìƒìœ„ ê¸°ì—¬ ìš”ì¸ (íì—…â†‘)]")
for k, v in contrib.head(TOP_N).items():
    print(f"  {k:30s} {v:+.5f}  (impact ~ {shap_to_pct_point(v):+.2f}%p)")

print("\n[ë¡œì»¬ SHAP í•˜ìœ„ ê¸°ì—¬ ìš”ì¸ (íì—…â†“)]")
for k, v in contrib.tail(TOP_N).items():
    print(f"  {k:30s} {v:+.5f}  (impact ~ {shap_to_pct_point(v):+.2f}%p)")

print("\nâœ… ì €ì¥ ì™„ë£Œ: one_store_explain.csv")
if COMPUTE_GLOBAL_DIST:
    print("âœ… ì €ì¥ ì™„ë£Œ: one_store_distribution.csv (ì „ì²´ ë¶„í¬ + ë°±ë¶„ìœ„)")

# (ì„ íƒ) ìƒìœ„ 10ê°œ ì˜í–¥ë„ í‘œ ê°„ë‹¨ ì¶œë ¥
print("\n[í™•ë¥  ë³€í™” ê¸°ì¤€ Top 10 ìš”ì¸ (%p)]")
tmp = out[["feature","impact_pct_point"]].copy().sort_values("impact_pct_point", ascending=False).head(10)
for _, r in tmp.iterrows():
    print(f"  {r['feature']:30s} {r['impact_pct_point']:+.2f}%p")

# =========================
# 8) NLP íŒŒì´í”„ë¼ì¸ (ë‹¨ì¼ ì‹ë‹¹)
# =========================

OUT_DIR_NLP.mkdir(exist_ok=True, parents=True)

# ëŒ€ìƒ business_id í™•ì •
biz_id = meta.get("business_id") if "business_id" in meta else (df.loc[idx, "business_id"] if "business_id" in df.columns else None)
if biz_id is None:
    raise ValueError("df11ì— business_id ì»¬ëŸ¼ì´ ì—†ì–´ NLP íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë©”íƒ€ & ì¹´í…Œê³ ë¦¬
c_cols = [c for c in df.columns if str(c).startswith("c_")]
state  = df.loc[idx, "state"] if "state" in df.columns else None

# ë¦¬ë·° ë¡œë“œ
reviews = pd.read_parquet(REVIEWS_PATH)
ensure_cols(reviews, {"business_id", "date", "text"}, "reviews")
reviews["date"] = pd.to_datetime(reviews["date"], errors="coerce")
reviews = reviews.dropna(subset=["date", "text"]).copy()
reviews["text"] = reviews["text"].astype(str)

# í”¼ì–´ ì„ ì •: ê°™ì€ ì£¼ + ì¹´í…Œê³ ë¦¬ 1ê°œ ì´ìƒ ê²¹ì¹¨
meta_simple = df[["business_id", "state"] + c_cols].copy()
row_c = df.loc[idx]
match_vec = [1 if row_c.get(c, 0) == 1 else 0 for c in c_cols]
peers = meta_simple[
    (meta_simple["state"] == state) &
    (meta_simple[c_cols].mul(match_vec, axis=1).sum(axis=1) >= 1) &
    (meta_simple["business_id"] != biz_id)
]["business_id"].tolist()

target_df = reviews[reviews["business_id"] == biz_id].copy()
peer_df   = reviews[reviews["business_id"].isin(peers)].copy()

if target_df.empty:
    raise ValueError("í•´ë‹¹ business_idì˜ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤ (reviews í™•ì¸).")
if peer_df.empty:
    if state and "state" in df.columns:
        other_ids = df[(df["state"] == state) & (df["business_id"] != biz_id)]["business_id"].tolist()
        peer_df = reviews[reviews["business_id"].isin(other_ids)].copy()
    if peer_df.empty:
        peer_df = reviews.sample(min(len(reviews), 5000), random_state=42).copy()

# ì €ì¥
target_df.to_csv(OUT_DIR_NLP / "target_reviews.csv", index=False, encoding="utf-8-sig")
peer_df.to_csv(OUT_DIR_NLP / "peer_reviews.csv", index=False, encoding="utf-8-sig")
df.loc[[idx]].to_csv(OUT_DIR_NLP / "target_meta.csv", index=False, encoding="utf-8-sig")

# ê°ì„± + ê¸¸ì´ + ì›”ë³„ íŠ¸ë Œë“œ
analyzer = SentimentIntensityAnalyzer()

def vsent(s):
    d = analyzer.polarity_scores(str(s))
    return d["compound"]

for dfx in (target_df, peer_df):
    dfx["sentiment"] = dfx["text"].apply(vsent)
    dfx["review_len"] = dfx["text"].str.split().apply(lambda x: len(x) if isinstance(x, list) else len(str(x).split()))
    dfx["ym"] = pd.to_datetime(dfx["date"]).dt.to_period("M").astype(str)

trend = (target_df.groupby("ym")
         .agg(avg_sentiment=("sentiment","mean"),
              avg_length=("review_len","mean"),
              n=("text","count"))
         .reset_index())
trend.to_csv(OUT_DIR_NLP / "monthly_trend.csv", index=False, encoding="utf-8-sig")

# ë¶€ì • ë¦¬ë·°ë§Œ n-gram â†’ ë¡œê·¸ì˜¤ì¦ˆ

def tokenize(s: str):
    s = s.lower()
    s = re.sub(r"[^a-z0-9\s\-']", " ", s)
    toks = [t for t in s.split() if 3 <= len(t) <= 20]
    return toks

def ngram_counts(texts, n=2):
    cnt = Counter()
    for t in texts:
        toks = tokenize(t)
        grams = zip(*[toks[i:] for i in range(n)])
        cnt.update([" ".join(g) for g in grams])
    return cnt

def log_odds(target_cnt: Counter, peer_cnt: Counter, k=1.0) -> pd.DataFrame:
    vocab = set(target_cnt) | set(peer_cnt)
    t_total = sum(target_cnt.values()) + k * len(vocab)
    p_total = sum(peer_cnt.values()) + k * len(vocab)
    rows = []
    for w in vocab:
        t = target_cnt.get(w, 0) + k
        p = peer_cnt.get(w, 0) + k
        lo = math.log((t/(t_total - t)) / (p/(p_total - p)))
        rows.append((w, t - k, p - k, lo))
    df_lo = pd.DataFrame(rows, columns=["ngram","count_target","count_peer","log_odds"])\
            .sort_values("log_odds", ascending=False)
    return df_lo

neg_thr = -0.05
nt = target_df[target_df["sentiment"] <= neg_thr]
np_ = peer_df[peer_df["sentiment"] <= neg_thr]

bi_t  = ngram_counts(nt["text"], n=2)
bi_p  = ngram_counts(np_["text"], n=2)
tri_t = ngram_counts(nt["text"], n=3)
tri_p = ngram_counts(np_["text"], n=3)

bi_lo  = log_odds(bi_t, bi_p, k=1.0)
tri_lo = log_odds(tri_t, tri_p, k=1.0)

bi_lo.to_csv(OUT_DIR_NLP / "logodds_bigram_neg.csv",   index=False, encoding="utf-8-sig")
tri_lo.to_csv(OUT_DIR_NLP / "logodds_trigram_neg.csv", index=False, encoding="utf-8-sig")

# ì¸¡ë©´(Aspect) ì ìˆ˜
aspects = {
    "service": ["service","server","staff","wait","rude","attitude","slow","attentive","friendly"],
    "taste":   ["taste","flavor","bland","salty","sweet","soggy","fresh","overcooked","undercooked","seasoning"],
    "price":   ["price","expensive","overpriced","cheap","worth","value","portion"],
    "clean":   ["clean","dirty","smell","sticky","sanitary","restroom","hair"],
    "speed":   ["slow","fast","wait","line","delay","quick","time"],
}
rows_aspect = []
for asp, kws in aspects.items():
    pattern = "|".join([re.escape(k) for k in kws])
    mask = target_df["text"].str.contains(pattern, case=False, na=False)
    sub = target_df[mask]
    rows_aspect.append({
        "aspect": asp,
        "n_reviews": len(sub),
        "avg_sentiment": sub["sentiment"].mean() if len(sub) else np.nan,
        "example": sub["text"].iloc[0][:200].replace("\n", " ") if len(sub) else ""
    })
aspect_df = pd.DataFrame(rows_aspect).sort_values("avg_sentiment")
aspect_df.to_csv(OUT_DIR_NLP / "aspect_scores.csv", index=False, encoding="utf-8-sig")

# ì˜ˆì‹œ ë¬¸ì¥ (ë¶€ì •/ê¸ì • ê° 10ê°œ)
ex_neg = target_df.sort_values("sentiment").head(10)[["date","sentiment","text"]]
ex_pos = target_df.sort_values("sentiment").tail(10)[["date","sentiment","text"]]
ex_neg.to_csv(OUT_DIR_NLP / "examples_negative.csv", index=False, encoding="utf-8-sig")
ex_pos.to_csv(OUT_DIR_NLP / "examples_positive.csv", index=False, encoding="utf-8-sig")

# ì›”ë³„ ê°ì„± ì°¨íŠ¸(ì„ íƒ)
if not trend.empty:
    plt.figure(figsize=(7,4))
    plt.plot(trend["ym"], trend["avg_sentiment"], marker="o")
    plt.xticks(rotation=60)
    plt.title("Monthly Sentiment (Target)")
    plt.tight_layout()
    plt.savefig(OUT_DIR_NLP / "monthly_sentiment.png", dpi=150)

# ìš”ì•½ ì¹´ë“œ
meta_series = df.loc[idx]
if not trend.empty:
    tmp = trend.copy()
    tmp["ym"] = pd.PeriodIndex(tmp["ym"], freq="M").to_timestamp()
    recent_3m = tmp.sort_values("ym").tail(3)["avg_sentiment"].mean()
else:
    recent_3m = np.nan

worst_aspect = ""
worst_aspect_sent = np.nan
if not aspect_df.empty and aspect_df["avg_sentiment"].notna().any():
    wr = aspect_df.sort_values("avg_sentiment").iloc[0]
    worst_aspect = wr["aspect"]
    worst_aspect_sent = float(wr["avg_sentiment"]) if pd.notna(wr["avg_sentiment"]) else np.nan

# ë¶ˆë§Œ Top í‚¤ì›Œë“œ (bi/tri í˜¼í•© ìƒìœ„ 5)
_top = (pd.concat([
            bi_lo.sort_values("log_odds", ascending=False).head(5).assign(n=2),
            tri_lo.sort_values("log_odds", ascending=False).head(5).assign(n=3),
        ], ignore_index=True)
        .sort_values(["log_odds"], ascending=False)
        .head(5))
complaints_str = "; ".join(_top["ngram"].tolist())

card = {
    "name": meta_series.get("name", ""),
    "state": meta_series.get("state", ""),
    "city": meta_series.get("city", ""),
    "stars": meta_series.get("stars", ""),
    "review_count": meta_series.get("review_count", ""),
    "stability_score": meta_series.get("stability_score", ""),
    "loyalty_score": meta_series.get("loyalty_score", ""),
    "reliability_score": meta_series.get("reliability_score", ""),
    "recent_3m_sentiment": recent_3m,
    "worst_aspect": worst_aspect,
    "worst_aspect_sent": worst_aspect_sent,
    "top_complaints": complaints_str,
    "pred_proba": proba,
    "score_simple": score_dict["score_simple"],
    "score_log": score_dict["score_log"],
}
if rank_pct is not None:
    card["risk_percentile"] = rank_pct
    card["risk_top_percent"] = 100.0 - rank_pct

pd.DataFrame([card]).to_csv(OUT_DIR_NLP / "nlp_summary_card.csv", index=False, encoding="utf-8-sig")

print("\nâœ… NLP ì™„ë£Œ: ì¶œë ¥ ê²½ë¡œ =", OUT_DIR_NLP.resolve())
print("- target_reviews.csv / peer_reviews.csv")
print("- monthly_trend.csv / monthly_sentiment.png")
print("- logodds_bigram_neg.csv / logodds_trigram_neg.csv")
print("- aspect_scores.csv")
print("- examples_negative.csv / examples_positive.csv")
print("- nlp_summary_card.csv")

try:
    r3 = f"{recent_3m:.3f}"
except Exception:
    r3 = "nan"

print(f"\nìš”ì•½: {card['name']}({card['state']}, {card['city']}) | ìµœê·¼3ê°œì›” ê°ì„±={r3} | "
      f"ìµœì•…ì¸¡ë©´={card['worst_aspect']}({card['worst_aspect_sent']}) | "
      f"ë¶ˆë§ŒTop: {card['top_complaints']}")